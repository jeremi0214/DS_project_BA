{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "---\n",
    "\n",
    "## Web scraping and analysis\n",
    "\n",
    "This Jupyter notebook includes some code to get you started with web scraping. We will use a package called `BeautifulSoup` to collect the data from the web. Once you've collected your data and saved it into a local `.csv` file you should start with your analysis.\n",
    "\n",
    "### Scraping data from Skytrax\n",
    "\n",
    "If you visit [https://www.airlinequality.com] you can see that there is a lot of data there. For this task, we are only interested in reviews related to British Airways and the Airline itself.\n",
    "\n",
    "If you navigate to this link: [https://www.airlinequality.com/airline-reviews/british-airways] you will see this data. Now, we can use `Python` and `BeautifulSoup` to collect all the links to the reviews and then to collect the text data on each of the individual review links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1\n",
      "   ---> 100 total reviews\n",
      "Scraping page 2\n",
      "   ---> 200 total reviews\n",
      "Scraping page 3\n",
      "   ---> 300 total reviews\n",
      "Scraping page 4\n",
      "   ---> 400 total reviews\n",
      "Scraping page 5\n",
      "   ---> 500 total reviews\n",
      "Scraping page 6\n",
      "   ---> 600 total reviews\n",
      "Scraping page 7\n",
      "   ---> 700 total reviews\n",
      "Scraping page 8\n",
      "   ---> 800 total reviews\n",
      "Scraping page 9\n",
      "   ---> 900 total reviews\n",
      "Scraping page 10\n",
      "   ---> 1000 total reviews\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://www.airlinequality.com/airline-reviews/british-airways\"\n",
    "pages = 10\n",
    "page_size = 100\n",
    "\n",
    "reviews = []\n",
    "\n",
    "for i in range(1, pages + 1):\n",
    "    print(f\"Scraping page {i}\")\n",
    "\n",
    "    # Create URL to collect links from paginated data\n",
    "    url = f\"{base_url}/page/{i}/?sortby=post_date%3ADesc&pagesize={page_size}\"\n",
    "\n",
    "    # Collect HTML data from this page\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse content\n",
    "    content = response.content\n",
    "    parsed_content = BeautifulSoup(content, 'html.parser')\n",
    "    for para in parsed_content.find_all(\"div\", {\"class\": \"text_content\"}):\n",
    "        reviews.append(para.get_text())\n",
    "    \n",
    "    print(f\"   ---> {len(reviews)} total reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Not Verified | Took a trip to Nashville with m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not Verified |  A nightmare journey courtesy o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>✅ Trip Verified | Absolutely atrocious. LHR-OR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>✅ Trip Verified | As someone who flies relentl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>✅ Trip Verified |   Flew with British Airways ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews\n",
       "0  Not Verified | Took a trip to Nashville with m...\n",
       "1  Not Verified |  A nightmare journey courtesy o...\n",
       "2  ✅ Trip Verified | Absolutely atrocious. LHR-OR...\n",
       "3  ✅ Trip Verified | As someone who flies relentl...\n",
       "4  ✅ Trip Verified |   Flew with British Airways ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe for all reviews\n",
    "df = pd.DataFrame()\n",
    "df[\"reviews\"] = reviews\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it to CSV file\n",
    "df.to_csv(\"BA_reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Now you have your dataset for this task! The loops above collected 1000 reviews by iterating through the paginated pages on the website. However, if you want to collect more data, try increasing the number of pages!\n",
    "\n",
    " The next thing that you should do is clean this data to remove any unnecessary text from each of the rows. For example, \"✅ Trip Verified\" can be removed from each row if it exists, as it's not relevant to what we want to investigate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "In this part, we will remove unnecessary text from each of the rows. We will use `NLTK` to help us clean the data and get tokenized. The cleaned texts will be saved into dataframe and then saved in another CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jeremychen/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jeremychen/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jeremychen/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/jeremychen/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/jeremychen/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download stopwords and punkt from nltk\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      Not Verified | Took a trip to Nashville with m...\n",
      "1      Not Verified |  A nightmare journey courtesy o...\n",
      "2      ✅ Trip Verified | Absolutely atrocious. LHR-OR...\n",
      "3      ✅ Trip Verified | As someone who flies relentl...\n",
      "4      ✅ Trip Verified |   Flew with British Airways ...\n",
      "                             ...                        \n",
      "995    ✅ Trip Verified |  Return flight to Dublin. Ou...\n",
      "996    ✅ Trip Verified |  Barbados to Gatwick. We boa...\n",
      "997    ✅ Trip Verified |  I would like to praise the ...\n",
      "998    ✅ Trip Verified | Madrid to London Heathrow. T...\n",
      "999    ✅ Trip Verified | BA762 Heathrow to Oslo I hav...\n",
      "Name: reviews, Length: 1000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Load CSV file and read the dataset\n",
    "reviews = pd.read_csv(\"BA_reviews.csv\")\n",
    "texts = reviews['reviews']\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Unnamed: 0                                            reviews  \\\n",
      "0             0  Not Verified | Took a trip to Nashville with m...   \n",
      "1             1  Not Verified |  A nightmare journey courtesy o...   \n",
      "2             2  ✅ Trip Verified | Absolutely atrocious. LHR-OR...   \n",
      "3             3  ✅ Trip Verified | As someone who flies relentl...   \n",
      "4             4  ✅ Trip Verified |   Flew with British Airways ...   \n",
      "..          ...                                                ...   \n",
      "995         995  ✅ Trip Verified |  Return flight to Dublin. Ou...   \n",
      "996         996  ✅ Trip Verified |  Barbados to Gatwick. We boa...   \n",
      "997         997  ✅ Trip Verified |  I would like to praise the ...   \n",
      "998         998  ✅ Trip Verified | Madrid to London Heathrow. T...   \n",
      "999         999  ✅ Trip Verified | BA762 Heathrow to Oslo I hav...   \n",
      "\n",
      "                                       cleaned_reviews  \n",
      "0    took trip nashville wife leisure break arrived...  \n",
      "1    nightmare journey courtesy british airway wors...  \n",
      "2    absolutely atrocious lhrordlhr roundtrip briti...  \n",
      "3    someone fly relentlessly british airway busine...  \n",
      "4    flew british airway club europe saturday 31st ...  \n",
      "..                                                 ...  \n",
      "995  return flight dublin outbound gallery north fi...  \n",
      "996  barbados gatwick boarded half full flight whil...  \n",
      "997  would like praise outstanding courtesy service...  \n",
      "998  madrid london heathrow second trip madrid lond...  \n",
      "999  ba762 heathrow oslo flown ba time last month c...  \n",
      "\n",
      "[1000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Removing verification tags\n",
    "texts = texts.str.strip(\"✅ Trip Verified |\")\n",
    "texts = texts.str.strip(\"Not Verified |\")\n",
    "\n",
    "# Function to clean text\n",
    "def preprocess(text):\n",
    "    # convert to lowercase and tokenize\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+', '', text) # remove urls\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # remove punctuations\n",
    "\n",
    "    # create English stop words list\n",
    "    en_stop = set(stopwords.words('english'))\n",
    "\n",
    "    # remove stop words from text\n",
    "    unstopped_words = [word for word in text.split() if word not in en_stop]\n",
    "\n",
    "    # lemmatize words\n",
    "    lemmatized_words = [lemma.lemmatize(word) for word in unstopped_words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "# Clean the texts\n",
    "cleaned_texts = texts.apply(preprocess)\n",
    "reviews['cleaned_reviews'] = cleaned_texts\n",
    "print(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned texts to CSV file\n",
    "reviews.to_csv(\"cleaned_BA_reviews.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
